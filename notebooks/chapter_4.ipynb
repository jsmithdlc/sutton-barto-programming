{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7 - Jack's Car Rental Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "def calculate_request_reward(cars_available, cars_requested):\n",
    "    if cars_requested > cars_available:\n",
    "        return -1\n",
    "    return cars_requested * 10  # $10 per requested car\n",
    "\n",
    "\n",
    "def compute_movement(n_cars_1, n_cars_2, n_moved, n_max=20):\n",
    "    # assumes that moved cars are effectively available in source location\n",
    "    return min(n_cars_1 - n_moved, n_max), min(n_cars_2 - n_moved, n_max)\n",
    "\n",
    "\n",
    "# computes triples\n",
    "def compute_possible_next_cars(\n",
    "    current_cars,\n",
    "    max_requested,\n",
    "    max_returned,\n",
    "    lambda_requested,\n",
    "    lambda_returned,\n",
    "    n_max=20,\n",
    "):\n",
    "    dist_req = poisson(mu=lambda_requested)\n",
    "    dist_ret = poisson(mu=lambda_returned)\n",
    "\n",
    "    # array that stores different outcomes, depending on requested and returned ammounts\n",
    "    next_cars = np.ones((max_requested, max_returned)) * current_cars\n",
    "    probs = np.zeros((max_requested, max_returned))\n",
    "    rewards = np.zeros((max_requested, max_returned))\n",
    "\n",
    "    for requested in range(max_requested):\n",
    "        for returned in range(max_returned):\n",
    "            next_cars[requested, returned] = np.clip(\n",
    "                next_cars[requested, returned] - requested + returned, 0, n_max\n",
    "            )\n",
    "            probs[requested, returned] = dist_req.pmf(requested) * dist_ret.pmf(\n",
    "                returned\n",
    "            )\n",
    "            rewards[requested, returned] = calculate_request_reward(\n",
    "                current_cars, requested\n",
    "            )\n",
    "    return np.stack([probs.flatten(), next_cars.flatten(), rewards.flatten()], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute a single step in the environment dynamics. Let's say we can have a maximum of 10 cars in each location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose we have 5 cars at location 1, 3 cars at location 2\n",
    "n_cars_1 = 5\n",
    "n_cars_2 = 3\n",
    "# policy tells us to move 2 cars from location_1 to location_2\n",
    "n_moved = 2\n",
    "n_cars_1, n_cars_2 = compute_movement(n_cars_1, n_cars_2, n_moved, n_max=10)\n",
    "\n",
    "# possible variations in reward and states\n",
    "next_1_values = compute_possible_next_cars(\n",
    "    n_cars_1, 10, 10, lambda_requested=3, lambda_returned=3\n",
    ")\n",
    "next_2_values = compute_possible_next_cars(\n",
    "    n_cars_2, 10, 10, lambda_requested=4, lambda_returned=2\n",
    ")\n",
    "\n",
    "sample = next_1_values[23, :]\n",
    "print(\n",
    "    f\"Number of cars in location 1 for next day (sample)\\n{'-'*20}\\nProbability: {sample[0]} | N_cars: {sample[1]} | Reward: {sample[2]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next state, along with the probability and reward, is determined by combining next values in location 1 and location 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class TabularValueFunction:\n",
    "    def __init__(self, nrows: int, ncols: int):\n",
    "        self.v = np.zeros((nrows + 1, ncols + 1))\n",
    "        self.states = list(\n",
    "            itertools.product(np.arange(nrows + 1), np.arange(ncols + 1))\n",
    "        )\n",
    "        self.state_space = (nrows + 1, ncols + 1)\n",
    "\n",
    "    def update_value(self, row, col, new_value):\n",
    "        self.v[row, col] = new_value\n",
    "\n",
    "    def get_value(self, row, col):\n",
    "        return self.v[row, col]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states\n",
    "    \n",
    "    def get_value_table(self):\n",
    "        return self.v\n",
    "\n",
    "\n",
    "class RowPolicy:\n",
    "    def __init__(self, state_space: tuple[int, int]):\n",
    "        self.state_space = state_space\n",
    "        self.policy_values = np.zeros(state_space, dtype=int)\n",
    "\n",
    "    def sample_action(self, row, col):\n",
    "        return self.policy_values[row, col]\n",
    "    \n",
    "    def update_action(self, row, col, new_action):\n",
    "        self.policy_values[row,col] = new_action\n",
    "\n",
    "    def get_policy_table(self):\n",
    "        return self.policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "class JacksCarProblem:\n",
    "    # poisson distribution for request and return values\n",
    "    lambda_request = (4, 3)\n",
    "    lambda_return = (3, 2)\n",
    "    # limit of cars in any location\n",
    "    max_cars = 20\n",
    "    # max car movements\n",
    "    max_mvnt = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        # request probability by location for upto 10 cars [CARS_REQUESTED, LOCATION]\n",
    "        self.request_probs = np.stack(\n",
    "            [\n",
    "                poisson(mu=self.lambda_request[0]).pmf(np.arange(10)),\n",
    "                poisson(mu=self.lambda_request[1]).pmf(np.arange(10)),\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "        # return probability by location for upto 10 cars [CARS_RETURNED, LOCATION]\n",
    "        self.return_probs = np.stack(\n",
    "            [\n",
    "                poisson(mu=self.lambda_return[0]).pmf(np.arange(10)),\n",
    "                poisson(mu=self.lambda_return[1]).pmf(np.arange(10)),\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "    def calculate_location_reward(self, cars_count:int, cars_requested:int):\n",
    "        # in case the requested carse in any location exceed the available ones\n",
    "        if cars_requested > cars_count:\n",
    "            return -10\n",
    "        # +$10 every requested car and -$3 every car moved \n",
    "        reward = cars_requested*10\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def __compute_location_step(self,cars:int, cars_displaced:int, location_id: int):\n",
    "        next_cars = np.zeros((10,10))\n",
    "        probs = np.zeros((10,10))\n",
    "        rewards = np.zeros((10,10))\n",
    "        \n",
    "        for cars_requested in range(10):\n",
    "            for cars_returned in range(10):\n",
    "                # compute probability of this number of cars requested and returned\n",
    "                prob_requested = self.request_probs[cars_requested,location_id]\n",
    "                prob_returned = self.return_probs[cars_returned, location_id]\n",
    "                probs[cars_requested, cars_returned] = prob_requested*prob_returned\n",
    "\n",
    "                # compute location reward based on available cars after displacement\n",
    "                reward = self.calculate_location_reward(cars-cars_displaced, cars_requested)\n",
    "                rewards[cars_requested, cars_returned] = reward\n",
    "\n",
    "                # update car number. Clip to range [0,20]\n",
    "                new_cars = min(max(cars - cars_displaced - cars_requested + cars_returned,0),20)\n",
    "                next_cars[cars_requested, cars_returned] = new_cars\n",
    "        return np.stack(\n",
    "            [\n",
    "                probs.flatten(), next_cars.flatten(), rewards.flatten()\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "    \n",
    "    def __combine_locations(self, location_1_triple: np.ndarray, location_2_triple: np.ndarray):\n",
    "        # combine values for all probable combinations of requested and returned cars in each location\n",
    "        probs = []\n",
    "        states = []\n",
    "        rewards = []\n",
    "        for location_1_result in location_1_triple:\n",
    "            for location_2_result in location_2_triple:\n",
    "                # compute this alternative probability\n",
    "                result_prob = location_1_result[0] * location_2_result[0]\n",
    "                new_state = (int(location_1_result[1]),int(location_2_result[1]))\n",
    "                # punish when any location has more requests than cars\n",
    "                if location_1_result[2] < 0  or location_2_result[2] < 0:\n",
    "                    result_reward = -10\n",
    "                else:\n",
    "                    result_reward = location_1_result[2] + location_2_result[2]\n",
    "                probs.append(result_prob)\n",
    "                states.append(new_state)\n",
    "                rewards.append(result_reward)\n",
    "        return {\n",
    "            \"probs\":np.array(probs),\n",
    "            \"states\":np.array(states),\n",
    "            \"rewards\":np.array(rewards)\n",
    "        }\n",
    "\n",
    "    \n",
    "    def compute_step(self,cars_count:tuple[int], cars_moved: int):\n",
    "        assert (cars_count[0]-cars_moved) >= 0 and (cars_count[1] + cars_moved) >= 0\n",
    "        \n",
    "        # compute step for each location\n",
    "        step_location_1 = self.__compute_location_step(cars_count[0],cars_moved,0)\n",
    "        step_location_2 = self.__compute_location_step(cars_count[1],-cars_moved,1)\n",
    "\n",
    "        # combine results for each location\n",
    "        combinations = self.__combine_locations(step_location_1, step_location_2)\n",
    "\n",
    "        # this should be included in rewards ($2 per car moved)\n",
    "        movement_cost = abs(cars_moved)*2\n",
    "        combinations[\"rewards\"] = combinations[\"rewards\"] - movement_cost\n",
    "\n",
    "        return combinations\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def policy_evaluation(\n",
    "    value_function: TabularValueFunction, policy: RowPolicy, env:JacksCarProblem, discount_rate: float = 0.9, theta: float = 0.001\n",
    "):\n",
    "    assert value_function.state_space == policy.state_space\n",
    "    print(\"Running policy evaluation ...\")\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state1, state2 in value_function.get_states():\n",
    "            old_value = value_function.get_value(state1, state2)\n",
    "            action = policy.sample_action(state1, state2)\n",
    "            combinations = env.compute_step((state1, state2),action)\n",
    "            v_sum = 0\n",
    "            for i in range(len(combinations[\"states\"])):\n",
    "                new_state = combinations[\"states\"][i]\n",
    "                prob = combinations[\"probs\"][i]\n",
    "                reward = combinations[\"rewards\"][i]\n",
    "                v_sum += prob*(reward + discount_rate*value_function.get_value(new_state[0],new_state[1]))\n",
    "            delta = max(delta, abs(old_value-v_sum))\n",
    "            value_function.update_value(state1, state2, v_sum)\n",
    "        print(f'Delta: {delta:.4f}' , end='\\r')\n",
    "        if delta < theta:\n",
    "            break\n",
    "    print(\"-> Finished policy evaluation! <-\")\n",
    "    return value_function\n",
    "\n",
    "def policy_improvement(policy: RowPolicy, value_function: TabularValueFunction, env: JacksCarProblem, discount_rate: float = 0.9):\n",
    "    policy_stable = True\n",
    "    print(\"Running policy improvement ...\")\n",
    "    for state1, state2 in tqdm(value_function.get_states()):\n",
    "        old_action = policy.sample_action(state1, state2)\n",
    "        # the number of cars moved from one location to another is restricted to available cars in each location\n",
    "        action_range = [max(-state2, -5), min(state1,5)]\n",
    "        best_action = old_action\n",
    "        best_value = 0\n",
    "        for action in range(action_range[0], action_range[1]+1):\n",
    "            combinations = env.compute_step((state1, state2),action)\n",
    "            action_value = 0\n",
    "            for i in range(len(combinations[\"states\"])):\n",
    "                new_state = combinations[\"states\"][i]\n",
    "                prob = combinations[\"probs\"][i]\n",
    "                reward = combinations[\"rewards\"][i]\n",
    "                action_value += prob*(reward + discount_rate*value_function.get_value(new_state[0],new_state[1]))\n",
    "            if action_value > best_value:\n",
    "                best_action = action\n",
    "        policy.update_action(state1, state2, best_action)\n",
    "        if old_action != best_action and value_function.get_value(state1, state2) != best_value:\n",
    "            policy_stable = False\n",
    "    return policy_stable, policy\n",
    "\n",
    "def policy_iteration(\n",
    "        policy: RowPolicy, \n",
    "        value_function: TabularValueFunction, \n",
    "        env: JacksCarProblem, \n",
    "        discount_rate: float = 0.9,\n",
    "        max_iterations: int = 1):\n",
    "    optimal_policy = False\n",
    "    step = 1\n",
    "    policies = []\n",
    "    value_functions = []\n",
    "    while not optimal_policy:\n",
    "        print(f\"Step {step}\")\n",
    "        # evaluate policy\n",
    "        value_function = policy_evaluation(value_function, policy, env, discount_rate=discount_rate)\n",
    "\n",
    "        # improve policy\n",
    "        optimal_policy, policy = policy_improvement(policy, value_function, env, discount_rate=discount_rate)\n",
    "\n",
    "        # store new policy and value function\n",
    "        policies.append(policy)\n",
    "        value_functions.append(value_function)\n",
    "\n",
    "        # max iterations\n",
    "        if step == max_iterations:\n",
    "            print(\"Reached max iterations!\")\n",
    "            break\n",
    "        step += 1\n",
    "        print(\"\\n\")\n",
    "    return policies, value_functions\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JacksCarProblem()\n",
    "initial_values = TabularValueFunction(20, 20)\n",
    "initial_policy = RowPolicy((21, 21))\n",
    "\n",
    "optimal_policy, optimal_values = policy_iteration(initial_policy, initial_values, env, discount_rate = 0.9, max_iterations=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "policy_id = 1\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(16,6))\n",
    "policy_1 = optimal_policy[policy_id].get_policy_table()\n",
    "sns.heatmap(policy_1,linewidths=0.5, linecolor='black',ax=axs[0])\n",
    "axs[0].set_title(f\"$\\pi_{policy_id+1}$\")\n",
    "\n",
    "policy_1 = optimal_values[policy_id].get_value_table()\n",
    "sns.heatmap(policy_1,linewidths=0.5, linecolor='black', cmap='viridis', ax=axs[1])\n",
    "axs[1].set_title(\"$V_{\\pi}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.9 - Value Iteration Gambler's Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RowValueFunction():\n",
    "    def __init__(self, n_states: int):\n",
    "        self.v = np.random.random(n_states+1)\n",
    "        self.states = np.arange(n_states+1)\n",
    "        self.v[0] = 0\n",
    "        self.v[-1] = 0\n",
    "\n",
    "    def update_value(self, state, new_value):\n",
    "        self.v[state] = new_value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return self.v[state]\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.states[1:-1]\n",
    "    \n",
    "    def get_value_table(self):\n",
    "        return self.v\n",
    "\n",
    "\n",
    "class RowPolicy:\n",
    "    def __init__(self, n_states: int):\n",
    "        self.policy_values = np.zeros(n_states+1, dtype=int)\n",
    "\n",
    "    def get_available_actions(self, state):\n",
    "        return np.arange(1,min(state,100-state)+1)\n",
    "    \n",
    "    def update_action(self, state, new_action):\n",
    "        self.policy_values[state] = new_action\n",
    "\n",
    "    def get_policy_values(self):\n",
    "        return self.policy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GamblersProblem:\n",
    "    def __init__(self, p_head:float=0.4):\n",
    "        self.p_head= p_head\n",
    "    \n",
    "    def compute_step(self, state: int, action:int):\n",
    "        # if state is 0, then we are in a terminal state\n",
    "        if state == 0:\n",
    "            return {\n",
    "                \"probs\":[1],\n",
    "                \"states\":[state],\n",
    "                \"rewards\":[0]\n",
    "            }\n",
    "        # if state is 100, then we are in a terminal state\n",
    "        elif state == 100:\n",
    "            return {\n",
    "                \"probs\":[1],\n",
    "                \"states\":[state],\n",
    "                \"rewards\":[0]\n",
    "            }\n",
    "        # otherwise, we are in a non-terminal state\n",
    "        else:\n",
    "            # compute probability of heads and tails\n",
    "            probs = [self.p_head, 1-self.p_head]\n",
    "            # compute possible states and rewards\n",
    "            states = [min(state+action,100), max(0,state-action)]\n",
    "            rewards = [1 if states[0] == 100 else 0,0]\n",
    "            return {\n",
    "                \"probs\":probs,\n",
    "                \"states\":states,\n",
    "                \"rewards\":rewards\n",
    "            }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env: GamblersProblem,value_function: RowValueFunction, policy: RowPolicy, theta: float = 0.001):\n",
    "    # initialize delta\n",
    "    values = []\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Update value function\n",
    "        for state in value_function.get_states():\n",
    "            old_value = value_function.get_value(state)\n",
    "            possible_actions = policy.get_available_actions(state)\n",
    "            max_val = 0\n",
    "            max_action = None\n",
    "            for action in possible_actions:\n",
    "                step = env.compute_step(state, action)\n",
    "                new_val = sum([prob*(step[\"rewards\"][i]+value_function.get_value(step[\"states\"][i])) for i, prob in enumerate(step[\"probs\"])])\n",
    "                if new_val > max_val:\n",
    "                    max_action = action\n",
    "                    max_val = new_val\n",
    "            value_function.update_value(state, max_val)\n",
    "            delta = max(delta, abs(old_value-max_val))\n",
    "        for state in value_function.get_states():\n",
    "            possible_actions = policy.get_available_actions(state)\n",
    "            max_val = 0\n",
    "            max_action = None\n",
    "            for action in possible_actions:\n",
    "                step = env.compute_step(state, action)\n",
    "                new_val = sum([prob*(step[\"rewards\"][i]+value_function.get_value(step[\"states\"][i])) for i, prob in enumerate(step[\"probs\"])])\n",
    "                if new_val > max_val:\n",
    "                    max_action = action\n",
    "                    max_val = new_val\n",
    "            policy.update_action(state, max_action)\n",
    "        print(f'Delta: {delta:.4f}',end='\\r')\n",
    "        values.append(value_function.get_value_table().copy())\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return values, policy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GamblersProblem(0.4)\n",
    "init_value_function = RowValueFunction(100)\n",
    "init_policy = RowPolicy(100)\n",
    "\n",
    "state_value_sweeps, new_policy = value_iteration(env, init_value_function, init_policy,1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plottable_sweeps = [0,1,2,9,35-1]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "states = init_value_function.get_states()\n",
    "policy_values = new_policy.get_policy_values()[1:-1]\n",
    "\n",
    "# plot optimal policy\n",
    "axs[0].bar(states, policy_values)\n",
    "axs[0].set_xlabel(\"Capital (state)\")\n",
    "axs[0].set_ylabel(\"Stake (action)\")\n",
    "# plot optimal value function\n",
    "\n",
    "for sweep in plottable_sweeps:\n",
    "    values = state_value_sweeps[sweep][1:-1]\n",
    "    axs[1].plot(states, values,label=sweep)\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Capital (state)\")\n",
    "axs[1].set_ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-experiments-r-JsmOJR-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
