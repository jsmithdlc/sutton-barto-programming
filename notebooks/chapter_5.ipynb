{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Racetrack (5.12)\n",
        "\n",
        "Action space is determined by adjustments to vertical and horizontal velocities"
      ],
      "metadata": {
        "id": "uZ8s9Q783D-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEbmBpAcbPvu"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "actions_horizontal = [-1,0,1]\n",
        "actions_vertical = [-1,0,1]\n",
        "action_space = list(itertools.product(actions_horizontal, actions_vertical))\n",
        "print(f\"Action space is: {len(action_space)} dimensional\")\n",
        "action_space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the problem, we generate a racetrack of 31 x 15 space. We consider the top part to be the starting line and the bottom right the finish line"
      ],
      "metadata": {
        "id": "6XWXjC6334dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_racetrack(rows, columns, lanewidth):\n",
        "    racetrack = np.ones((rows, columns), dtype=np.uint8)\n",
        "    racetrack[:rows-lanewidth,lanewidth:] = 0\n",
        "    return racetrack\n",
        "\n",
        "racetrack_ex = create_racetrack(31,20,8)\n",
        "plt.imshow(racetrack_ex)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ljEVle7H33FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7qhK7O0bPvw"
      },
      "source": [
        "Let's simulate a starting position at (0,3) which is the bottom of the map. Valid runway is shown in yellow with sample starting position represented in the red dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzs3O-hebPvz"
      },
      "outputs": [],
      "source": [
        "# We need this helper function to plot the dot over the racetrack considering that\n",
        "# the plt.imshow inverts the coordinate of reference\n",
        "def plot_dot_over_racetrack(row, column, racetrack):\n",
        "  plt.imshow(racetrack)\n",
        "  plt.plot(column, row, 'ro')\n",
        "  plt.show()\n",
        "\n",
        "start_pos_ex = (0,3)\n",
        "plot_dot_over_racetrack(*start_pos_ex, racetrack_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can implement functions for getting the next position of the car based on its current velocity and position. So, for the previous starting position and a velocity of (0,0) this leads to the following"
      ],
      "metadata": {
        "id": "NUaJExME7vf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_position(pos_row, pos_column, vel_row, vel_col):\n",
        "    return pos_row + vel_row, pos_column + vel_col\n",
        "\n",
        "next_pos_ex = next_position(*start_pos_ex,3,0)\n",
        "plot_dot_over_racetrack(*next_pos_ex, racetrack_ex)"
      ],
      "metadata": {
        "id": "su08weaT8hEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also helpful to have a way to know if the next position, based on current position and velocity, would lead to a boundary crash. Following the previous example, if we had a velocity of (0, 5) following the last position, it would crash with the boundary."
      ],
      "metadata": {
        "id": "3-tKZxZ79fRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PossibleOutcomes:\n",
        "  crash = 0\n",
        "  finish = 1\n",
        "  inrace = 2\n",
        "\n",
        "# we assume that a diagonal trajectory near the finish line, that ends with\n",
        "# the car beyond the racetrack height is invalid, to simplify\n",
        "def check_next_condition(pos_row, pos_column, vel_row, vel_column, racetrack):\n",
        "  next_row, next_col = next_position(pos_row, pos_column, vel_row, vel_column)\n",
        "  in_height = next_row < racetrack.shape[0] and next_row >= 0\n",
        "  if (in_height and next_col >= racetrack.shape[1]-1 and\n",
        "      racetrack[next_row][racetrack.shape[1]-1] == 1):\n",
        "    return PossibleOutcomes.finish\n",
        "  in_width = next_col < racetrack.shape[1] and next_col >= 0\n",
        "  if not in_height or not in_width:\n",
        "    return PossibleOutcomes.crash\n",
        "  elif racetrack[next_row][next_col] == 0:\n",
        "    return PossibleOutcomes.crash\n",
        "  return PossibleOutcomes.inrace\n",
        "\n",
        "crash_ex = check_next_condition(*next_pos_ex, 0, 5, racetrack_ex)\n",
        "print(f\"Car at position {next_pos_ex} moved by velocity (0,5) crashes? {crash_ex==PossibleOutcomes.crash}\")\n",
        "not_crash_ex = check_next_condition(*next_pos_ex,0,2, racetrack_ex)\n",
        "print(f\"Car at position {next_pos_ex} moved by velocity (0,2) crashes? {not_crash_ex==PossibleOutcomes.crash}\")"
      ],
      "metadata": {
        "id": "i_XIdGwM9yca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy\n",
        "\n",
        "To implement the policy, the state will be represented as:\n",
        "\n",
        "`[pos_row, pos_col, vel_row, vel_col]`\n",
        "\n",
        "And the agent can take an action as:\n",
        "\n",
        "`[acc_row, acc_col]`\n",
        "\n",
        "We proceed by implementing a helper function to create the action space"
      ],
      "metadata": {
        "id": "5PGiDkwWqVfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_action_space():\n",
        "  actions_horizontal = [-1,0,1]\n",
        "  actions_vertical = [-1,0,1]\n",
        "  action_space = list(itertools.product(actions_horizontal, actions_vertical))\n",
        "  return action_space\n",
        "\n",
        "action_space_rt = create_action_space()\n",
        "print(f\"Action space is: {action_space_rt}\")"
      ],
      "metadata": {
        "id": "UEJpU4SJ0ouP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, since velocities should be both in the range `]0,5]` except for the initial state, it is useful to create a helper function to filter to all valid actions for a given state (given by velocities)"
      ],
      "metadata": {
        "id": "Tb988Vhv4LFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_valid_action(velocities, accelerations):\n",
        "    new_row_vel = velocities[0] + accelerations[0]\n",
        "    new_col_vel = velocities[1] + accelerations[1]\n",
        "    if new_row_vel == 0 and new_col_vel == 0:\n",
        "      return False\n",
        "    elif new_row_vel < 0 or new_col_vel < 0:\n",
        "      return False\n",
        "    elif new_row_vel >= 5 or new_col_vel >= 5:\n",
        "      return False\n",
        "    return True\n",
        "\n",
        "def init_actions_for_state(state, action_space):\n",
        "    valid_actions = []\n",
        "    for action in action_space:\n",
        "      if check_if_valid_action(state[2:],action):\n",
        "        valid_actions.append(action)\n",
        "    return valid_actions\n",
        "\n",
        "def action_equality(ac1, ac2):\n",
        "    for i1, i2 in zip(ac1, ac2):\n",
        "      if i1 != i2:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "valid_actions_init = init_actions_for_state([0,0,0,0], action_space_rt)\n",
        "print(f\"Valid actions for an initial state of: [0,0,0,0] are: {valid_actions_init}\")\n"
      ],
      "metadata": {
        "id": "IFowZsLw1Tyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It follows a policy implementation for the e-greedy monte carlo"
      ],
      "metadata": {
        "id": "j6okPUHGddxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "def build_state_key(state):\n",
        "  return \"_\".join([str(s) for s in state])\n",
        "\n",
        "class FirstVisitMCPolicy:\n",
        "    \"\"\"\n",
        "    Implements Monte Carlo e-greedy policy\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "        self.action_space = action_space\n",
        "        self.policy = {}\n",
        "\n",
        "    def _init_state_policy(self, state):\n",
        "        valid_actions = init_actions_for_state(state, self.action_space)\n",
        "        self.policy[build_state_key(state)] = {\n",
        "            \"actions\":valid_actions,\n",
        "            \"probs\":[1/len(valid_actions) for _ in valid_actions]\n",
        "        }\n",
        "\n",
        "    def _compute_new_policy_value(self, action, max_action, n_actions):\n",
        "        if not action_equality(action, max_action):\n",
        "            return self.epsilon/n_actions\n",
        "        return 1 - self.epsilon + self.epsilon/n_actions\n",
        "\n",
        "    def set_epsilon(self, epsilon):\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def update(self, state, max_action):\n",
        "        state_key = build_state_key(state)\n",
        "        n_actions = len(self.policy[state_key][\"actions\"])\n",
        "        for ac_id, action in enumerate(self.policy[state_key][\"actions\"]):\n",
        "          self.policy[state_key][\"probs\"][ac_id] = self._compute_new_policy_value(\n",
        "              action, max_action, n_actions\n",
        "          )\n",
        "\n",
        "    def sample_action(self, state):\n",
        "        state_key = build_state_key(state)\n",
        "        if state_key not in self.policy:\n",
        "          self._init_state_policy(state)\n",
        "        action_probs = self.policy[state_key][\"probs\"]\n",
        "        action_indices = [i for i in range(len(action_probs))]\n",
        "        sampled_action_id = np.random.choice(action_indices, p = action_probs)\n",
        "        return self.policy[state_key][\"actions\"][sampled_action_id]\n",
        "\n",
        "\n",
        "# For an initial state of [0,0,0,0] there are 3 possible actions:\n",
        "# (0,1) - (1,0) - (1,1)\n",
        "agent = FirstVisitMCPolicy(action_space_rt, 0.1)\n",
        "state_example = [0,0,0,0]\n",
        "sampled_action = agent.sample_action(state_example)\n",
        "print(f\"Sampled action: {sampled_action} for state: {state_example}\")\n",
        "print(\"Policy for that state:\")\n",
        "# Action probabilities are initialized to uniform distribution\n",
        "pprint.pprint(agent.policy['0_0_0_0'])"
      ],
      "metadata": {
        "id": "VNXc57RzoIfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.update(state_example, [1,0])\n",
        "pprint.pprint(agent.policy[\"0_0_0_0\"])\n",
        "assert np.isclose(sum(agent.policy[\"0_0_0_0\"][\"probs\"]),1,1e-12)"
      ],
      "metadata": {
        "id": "KiTnoVR7a3n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action-Value Function\n",
        "\n",
        "We implement a class for the action value function"
      ],
      "metadata": {
        "id": "AE42svA74lbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class QFunction:\n",
        "  def __init__(self, action_space):\n",
        "    self.values = {}\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def _init_q_values(self, state):\n",
        "    valid_actions = init_actions_for_state(state, self.action_space)\n",
        "    #action_values = [random.randint(1,10) for a in valid_actions]\n",
        "    action_values = [0 for a in valid_actions]\n",
        "    self.values[build_state_key(state)] = {\n",
        "        \"actions\":valid_actions,\n",
        "        \"values\":action_values\n",
        "    }\n",
        "\n",
        "  def _get_action_id(self, actions, action):\n",
        "    for id, act in enumerate(actions):\n",
        "      if action_equality(action,act):\n",
        "        return id\n",
        "\n",
        "\n",
        "  def get_max_action(self, state):\n",
        "    s_key = build_state_key(state)\n",
        "    if s_key not in self.values:\n",
        "      self._init_q_values(state)\n",
        "    actions = self.values[s_key][\"actions\"]\n",
        "    action_values = self.values[s_key][\"values\"]\n",
        "    return actions[np.argmax(action_values)]\n",
        "\n",
        "  def get_value(self, state, action):\n",
        "    s_key = build_state_key(state)\n",
        "    if s_key not in self.values:\n",
        "      self._init_q_values(state)\n",
        "    act_id = self._get_action_id(self.values[s_key][\"actions\"], action)\n",
        "    if act_id is None:\n",
        "      raise ValueError(f\"No action: {action} registered for state: {state}\")\n",
        "\n",
        "  def update(self, state, action, new_value):\n",
        "    s_key = build_state_key(state)\n",
        "    if s_key not in self.values:\n",
        "      self._init_q_values(state)\n",
        "    action_id = self._get_action_id(self.values[s_key][\"actions\"],action)\n",
        "    if action_id is None:\n",
        "      raise ValueError(f\"No action: {action} is recorded for state: {state}\")\n",
        "    self.values[s_key][\"values\"][action_id] = new_value\n",
        "\n",
        "\n",
        "q_example = QFunction(action_space_rt)\n",
        "# we initialize action and values for initial state\n",
        "q_example.get_value(state_example, (0,1))\n",
        "pprint.pprint(q_example.values)\n",
        "# we sample maximal action for that state\n",
        "print(f\"Maximal action is {q_example.get_max_action(state_example)}\")\n",
        "# if we update action (1,0) with a value of 100, it should become maximal action\n",
        "q_example.update(state_example, (1,0), 100)\n",
        "assert q_example.get_max_action(state_example) == (1,0)"
      ],
      "metadata": {
        "id": "UupNKetUtK3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RaceTrackEnv\n",
        "\n",
        "We create a racetrack environment for generating episodes according to the rules"
      ],
      "metadata": {
        "id": "5k75yYeejNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class RaceTrackEnv:\n",
        "  def __init__(self, width, height, lanewidth):\n",
        "    self.rows = height\n",
        "    self.columns = width\n",
        "    self.lanewidth = lanewidth\n",
        "    self.racetrack = create_racetrack(height, width, lanewidth)\n",
        "    self.reset()\n",
        "\n",
        "  def _select_random_init_state(self):\n",
        "    col =  random.randint(0,self.lanewidth-1)\n",
        "    return [0, col, 0, 0]\n",
        "\n",
        "  def _update_last_state(self, row_acc, col_acc):\n",
        "    new_row_vel = self.last_state[2] + row_acc\n",
        "    new_col_vel = self.last_state[3] + col_acc\n",
        "    new_row = self.last_state[0] + new_row_vel\n",
        "    new_col = self.last_state[1] + new_col_vel\n",
        "    self.last_state = [new_row, new_col, new_row_vel, new_col_vel]\n",
        "\n",
        "  def reset(self):\n",
        "    self.last_state = self._select_random_init_state()\n",
        "    return self.last_state\n",
        "\n",
        "  def step(self, action) -> tuple[tuple, int, bool]:\n",
        "    \"\"\"\n",
        "    return state, reward, done\n",
        "    \"\"\"\n",
        "    new_row_vel = self.last_state[2] + action[0]\n",
        "    new_col_vel = self.last_state[3] + action[1]\n",
        "    next_condition = check_next_condition(\n",
        "        self.last_state[0],\n",
        "        self.last_state[1],\n",
        "        self.last_state[2],\n",
        "        self.last_state[3],\n",
        "        self.racetrack\n",
        "    )\n",
        "    if next_condition == PossibleOutcomes.crash:\n",
        "      self.reset()\n",
        "      return self.last_state, -1, False\n",
        "    if next_condition == PossibleOutcomes.finish:\n",
        "      return self.last_state, 0, True\n",
        "    self._update_last_state(*action)\n",
        "    return self.last_state, -1, False\n",
        "\n",
        "env_test = RaceTrackEnv(40,30,10)\n",
        "print(f\"Initial state: {env_test.last_state}\")\n",
        "env_test.step((1,1))\n",
        "print(f\"New state: {env_test.last_state} after acceleration of (1,1)\")\n"
      ],
      "metadata": {
        "id": "U1oH4cZwjV8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trajectory Plotter\n",
        "\n",
        "Sort of a more sophisticated version of the plotting function implemented earlier, this time to plot the whole trajectory of the car"
      ],
      "metadata": {
        "id": "JH_4FVrZqTqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_trajectory(trajectory, racetrack):\n",
        "  rows, cols = [], []\n",
        "  for s in trajectory:\n",
        "    rows.append(s[0])\n",
        "    cols.append(s[1])\n",
        "  plt.imshow(racetrack, cmap=\"bone\")\n",
        "  plt.plot(cols, rows, '--y')\n",
        "  plt.plot(cols[0],rows[0],\".g\")\n",
        "  plt.plot(cols[-1], rows[-1],\".r\")\n",
        "  plt.show()\n",
        "\n",
        "plot_trajectory([[0,0],[1,0],[1,1],[6,0],[6,2]],racetrack_ex)"
      ],
      "metadata": {
        "id": "6Ueneu7PqdAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Episode loop helper\n",
        "\n",
        "Function to run an episode given the racetrackenv and policy\n",
        "\n"
      ],
      "metadata": {
        "id": "6Z8lqIOkiBLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, policy, max_steps = 1e6) -> tuple[list[tuple],list[tuple],list[int]]:\n",
        "  done = False\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  trajectory = []\n",
        "  state = env.reset()\n",
        "  n_steps = 0\n",
        "  while not done and n_steps < max_steps:\n",
        "    action = policy.sample_action(state)\n",
        "    actions.append(action)\n",
        "    trajectory.append(state)\n",
        "    state, rew, done = env.step(action)\n",
        "    rewards.append(rew)\n",
        "    n_steps += 1\n",
        "  return done, trajectory, actions, rewards\n",
        "\n",
        "\n",
        "\n",
        "env_test = RaceTrackEnv(30,60,10)\n",
        "policy_test = FirstVisitMCPolicy(create_action_space(),epsilon = 1e-3)\n",
        "\n",
        "trajectory_test, actions_test, rewards_test = run_episode(env_test, policy_test)\n",
        "assert len(trajectory_test) == len(actions_test)\n",
        "assert len(actions_test) == len(rewards_test)\n",
        "plot_trajectory(trajectory_test, env_test.racetrack)"
      ],
      "metadata": {
        "id": "nCpvS0_sYoya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On-policy first-visit MC Control"
      ],
      "metadata": {
        "id": "dqiGympdv8AC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_visited_dict(state_list, action_list):\n",
        "  visited = {}\n",
        "  for i, (state,action) in enumerate(zip(state_list, action_list)):\n",
        "    key = build_state_key(state + list(action))\n",
        "    if key not in visited:\n",
        "      visited[key] = i\n",
        "  return visited\n",
        "\n",
        "states_t = [[0,0,0,0],[0,1,0,0],[0,0,0,0]]\n",
        "actions_t = [[1,0],[0,1],[1,0]]\n",
        "visited_t = create_visited_dict(states_t, actions_t)\n",
        "pprint.pprint(visited_t)\n",
        "assert len(visited_t) == 2"
      ],
      "metadata": {
        "id": "q8RH8k-0zAhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "DISCOUNT_FACTOR = 0.95\n",
        "N_EPISODES = 100\n",
        "PLOT_FREQ = 10\n",
        "INITIAL_EPSILON = 0.4\n",
        "\n",
        "env = RaceTrackEnv(30,60,10)\n",
        "policy = FirstVisitMCPolicy(create_action_space(),epsilon = INITIAL_EPSILON)\n",
        "q_function = QFunction(create_action_space())\n",
        "returns = {}\n",
        "\n",
        "for episode in range(N_EPISODES):\n",
        "  start_time = time.time()\n",
        "  terminated, states, actions, rewards = run_episode(env, policy)\n",
        "  visited = create_visited_dict(states, actions)\n",
        "  last_rew = 0\n",
        "  for t in range(len(states)-1,-1,-1):\n",
        "    # recompute last reward\n",
        "    last_rew = last_rew*DISCOUNT_FACTOR + rewards[t]\n",
        "    # check if state action appears earlier\n",
        "    state = states[t]\n",
        "    action = actions[t]\n",
        "    key = build_state_key(state + list(action))\n",
        "    if key in visited and visited[key] < t:\n",
        "      continue\n",
        "\n",
        "    # create return entry for state_action key\n",
        "    if key not in returns:\n",
        "      returns[key] = []\n",
        "\n",
        "    # add new return to this state-action pair\n",
        "    returns[key].append(last_rew)\n",
        "    # update q function and policy\n",
        "    q_function.update(states[t],actions[t],np.mean(returns[key]))\n",
        "\n",
        "    # find maximal action given new q function\n",
        "    max_action = q_function.get_max_action(state)\n",
        "\n",
        "    # update policy\n",
        "    policy.update(state, max_action)\n",
        "\n",
        "\n",
        "  # update epsilon as training progresses\n",
        "  end_time = time.time()\n",
        "  finish_msg = \"SUCCESS\" if terminated else \"TIMEOUT\"\n",
        "  print(f\"Episode: {episode} | {end_time-start_time} seconds | [{finish_msg}] | {policy.epsilon} epsilon\")\n",
        "\n",
        "  new_epsilon = INITIAL_EPSILON*((N_EPISODES-episode-1)/N_EPISODES)\n",
        "  policy.set_epsilon(new_epsilon)\n",
        "  if episode % PLOT_FREQ:\n",
        "      plot_trajectory(states, env.racetrack)\n",
        "\n"
      ],
      "metadata": {
        "id": "GVLqzz-Ov7Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VNJHWpRHGkiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}